{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score, roc_auc_score, hamming_loss, make_scorer\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import json\n",
    "import warnings\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"XMLData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlfiles = os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Linear Regression cannot be used for this problem because it is a regression algorithm and this is a multiclass and multilabel classification problem. I've added the code for Linear Regression in every question asked with an exception message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract Pandas Dataframe from XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(files):\n",
    "    dataframe_list = []\n",
    "    for index, file in enumerate(files):\n",
    "        dict_for_df = {}\n",
    "        if file[-4:] == \".xml\":\n",
    "            try:\n",
    "                with open(file) as f:\n",
    "                    data = f.read()\n",
    "                soup = BeautifulSoup(data, \"lxml\")\n",
    "                dict_for_df[\"headline\"] = soup.find(\"headline\").text\n",
    "                dict_for_df[\"text\"] = soup.find(\"text\").text\n",
    "                codes = soup.find(\"codes\", {\"class\": \"bip:topics:1.0\"})\n",
    "                dict_for_df[\"bip:topics\"] = [each_code[\"code\"] for each_code in codes.find_all(\"code\")]\n",
    "                dict_for_df[\"dc.date.pubished\"] = soup.find(\"dc\", {\"element\": \"dc.date.published\"})[\"value\"]\n",
    "                dict_for_df[\"itemid\"] = soup.find(\"newsitem\")[\"itemid\"]\n",
    "                dict_for_df[\"XMLfilename\"] = file\n",
    "                dataframe_list.append(dict_for_df)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    with open(\"../xmldata.json\", \"w\") as f:\n",
    "        json.dump(dataframe_list, f)\n",
    "    return pd.DataFrame(dataframe_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_df = get_dataframe(xmlfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_json(\"../xmldata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract all possible values of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_values(df, column_name=\"bip:topics\"):\n",
    "     return {each_value for each_row in df[column_name].values for each_value in each_row}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E11', 'C32', 'MCAT', 'C173', 'M13', 'G159', 'C31', 'G157', 'GCAT', 'E12', 'E141', 'E311', 'C312', 'C22', 'GREL', 'G15', 'C24', 'G154', 'E121', 'GWELF', 'C331', 'GVIO', 'C151', 'GSPO', 'GOBIT', 'M132', 'GDIS', 'G153', 'C42', 'E13', 'E513', 'C182', 'GFAS', 'E41', 'GCRIM', 'E61', 'G151', 'GVOTE', 'C152', 'E211', 'M142', 'C171', 'ECAT', 'G155', 'GENT', 'C21', 'GDIP', 'GHEA', 'E312', 'GENV', 'C311', 'G152', 'E71', 'M12', 'C411', 'E411', 'GSCI', 'C41', 'M11', 'C11', 'GODD', 'E21', 'E313', 'GPRO', 'GJOB', 'E212', 'C18', 'CCAT', 'C12', 'E14', 'GTOUR', 'E142', 'C313', 'C181', 'M141', 'C172', 'C14', 'C174', 'C13', 'E512', 'E143', 'C15', 'GDEF', 'M14', 'E131', 'G158', 'E511', 'C17', 'E31', 'E51', 'GWEA', 'C34', 'C33', 'M131', 'C23', 'C183', 'C16', 'GPOL', 'G156', 'C1511', 'E132', 'M143'}\n"
     ]
    }
   ],
   "source": [
    "topics = get_unique_values(text_df, column_name=\"bip:topics\")\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling a regex pattern out of stopwords\n",
    "stopwords_pattern = re.compile(r'\\b({})\\b'.format(\"|\".join(stopwords_list)), re.I)\n",
    "# Compiling a regex pattern to detect anything other than non-alphabets\n",
    "non_alpha_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "# Creating object for stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Function to lemmatize the words\n",
    "lemmatizer_fn = lambda word: lemmatizer.lemmatize(word)\n",
    "def clean_text(unclean_text):\n",
    "    unclean_text = stopwords_pattern.sub('', unclean_text)\n",
    "    unclean_text = non_alpha_pattern.sub('', unclean_text)\n",
    "    unclean_tokens = word_tokenize(unclean_text)\n",
    "    clean_tokens = list(map(lemmatizer_fn, unclean_tokens))\n",
    "    clean_text = \" \".join(clean_tokens)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"clean_text\"] = text_df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Shares National Australia Bank Ltd jumped cent...\n",
       "1        Omitted YEAR END DECEMBER AUDITED NET REVENUES...\n",
       "2        France urged Israel Monday stick Oslo peace ac...\n",
       "3        former Mexican deputy attorney general Friday ...\n",
       "4        Krupp close least part Dortmund steel plant st...\n",
       "                               ...                        \n",
       "48252    masked Zapatista rebel leader made rare appear...\n",
       "48253    CIA investigating whether senior officer agenc...\n",
       "48254    Three Months Ended Twelve Months Ended Decembe...\n",
       "48255    Snapshot P GMT FUTURES CASH YIELD DAY BILLS JU...\n",
       "48256    Bahrain state security court said listen day f...\n",
       "Name: clean_text, Length: 48257, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text_df):\n",
    "    main_df = pd.DataFrame()\n",
    "    # Combination of CountVectorizer and TfidfTransformer\n",
    "    tf_idf = TfidfVectorizer()\n",
    "    X = tf_idf.fit_transform(text_df[\"clean_text\"])\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(text_df[\"bip:topics\"])\n",
    "    # Pandas Dataframe containing features and labels according to requirement\n",
    "    main_df[\"features\"], main_df[\"labels\"] = list(X.toarray()), list(y)\n",
    "    return main_df, X, y, tf_idf, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df, X, y, tf_idf, mlb = extract_features(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48252</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48253</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48254</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48255</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48256</th>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48257 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                features  \\\n",
       "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "...                                                  ...   \n",
       "48252  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "48253  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "48254  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "48255  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "48256  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                  labels  \n",
       "0      [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, ...  \n",
       "1      [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "...                                                  ...  \n",
       "48252  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "48253  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "48254  [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "48255  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "48256  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[48257 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df[[\"features\", \"labels\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas Dataframe to sparse matrix\n",
    "X = sp.sparse.csr_matrix(np.array(list(main_df[\"features\"])))\n",
    "# Conver Pandas Dataframe to numpy ndarray\n",
    "y = np.array(list(main_df[\"labels\"]))\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_holdout_evaluation(X_train, y_train, X_test, y_test, model_type=\"rf\"):\n",
    "    if model_type==\"rf\":\n",
    "        model = RandomForestClassifier()\n",
    "    if model_type==\"lr\":\n",
    "        model = LinearRegression()\n",
    "    if model_type==\"mlp\":\n",
    "        model = MLPClassifier()\n",
    "    if model_type==\"dt\":\n",
    "        model = DecisionTreeClassifier()\n",
    "    if model_type==\"svm\":\n",
    "        model = SVC()\n",
    "    try:\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "    except:\n",
    "        print(\"Please use classification algorithm.\")\n",
    "        return None\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(X, y, cv=5, model_type=\"rf\"):\n",
    "    if model_type==\"rf\":\n",
    "        model = RandomForestClassifier()\n",
    "    if model_type==\"lr\":\n",
    "        model = LinearRegression()\n",
    "    if model_type==\"mlp\":\n",
    "        model = MLPClassifier()\n",
    "    if model_type==\"dt\":\n",
    "        model = DecisionTreeClassifier()\n",
    "    if model_type==\"svm\":\n",
    "        model = SVC(kernel=\"linear\")\n",
    "    try:\n",
    "        model = OneVsRestClassifier(model)\n",
    "        scores = cross_val_score(model, X, y, cv=cv)\n",
    "    except Exception as e:\n",
    "        print(\"Please use classification algorithm.\", e)\n",
    "        return None\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models_cross_val_scores = {\"mlp\":None, \"rf\":None, \"dt\":None, \"svm\":None}\n",
    "models_cross_val_scores = {key: perform_cross_validation(X_train, y_train, model_type=key) \n",
    "                                       for key, value in models_cross_val_scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt': array([0.025, 0.025, 0.05 , 0.025, 0.05 ]),\n",
       " 'mlp': array([0.05 , 0.125, 0.15 , 0.025, 0.1  ]),\n",
       " 'rf': array([0.   , 0.   , 0.025, 0.   , 0.   ]),\n",
       " 'svm': array([0.   , 0.025, 0.075, 0.025, 0.075])}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_cross_val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for key, value in models_cross_val_scores.items():\n",
    "    for x in models_cross_val_scores[key]:\n",
    "        df.append([key, x])\n",
    "df = pd.DataFrame(df, columns=[\"classifier\", \"cross_val_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box plot to display cross val scores of different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14b02f550>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=\"classifier\", y=\"cross_val_score\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasons to perform cross validation (KFold cross validation):\n",
    "- The purpose of cross validation is to check the performance of a machine learning model on unseen data.\n",
    "- This also detects overfitting in cases where the machine learning model has high variance and picks up the unwanted nuances in the training data. This leads to good performance on training data whereas the performance is bad on testing data.\n",
    "- In cases of time crunch or a huge dataset, hold-out evaluation can be used. In this method, the dataset is split into training and testing data and the model is trained on training data. However, this method relies heavily on how the data is split and might not give a good measure of how good a model is. On the other hand, KFold cross validation performs holdout evaluation across k splits of the dataset and this gives the model the chance to explore the whole dataset. This gives a better estimate of how good the model is.\n",
    "\n",
    "And, I will be using Kfold cross validation for my use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to return a trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_trained_classifier(X=None, y=None, model_type=\"rf\", params=None, train=0):\n",
    "    if model_type==\"rf\":\n",
    "        model = RandomForestClassifier(n_estimators = params[\"n_estimators\"], \n",
    "                                       min_samples_split = params[\"min_samples_split\"],\n",
    "                                        min_samples_leaf = params[\"min_samples_leaf\"],\n",
    "                                      criterion = params[\"criterion\"]) if params else RandomForestClassifier()\n",
    "    elif model_type==\"lr\":\n",
    "        model = LinearRegression()\n",
    "    elif model_type==\"mlp\":\n",
    "        model = MLPClassifier(activation = params[\"activation\"], \n",
    "                              solver = params[\"solver\"],\n",
    "                             alpha = params[\"alpha\"],\n",
    "                             learning_rate = params[\"learning_rate\"]) if params else MLPClassifier()\n",
    "    elif model_type==\"dt\":\n",
    "        model = DecisionTreeClassifier(criterion = params[\"criterion\"], \n",
    "                                       min_samples_split = params[\"min_samples_split\"],\n",
    "                                      max_depth = params[\"max_depth\"]) if params else DecisionTreeClassifier()\n",
    "    elif model_type==\"svm\":\n",
    "        model = SVC(C=params[\"C\"], gamma=params[\"gamma\"], kernel=params[\"kernel\"]) if params else SVC(kernel=\"linear\")\n",
    "    try:\n",
    "        if train:\n",
    "            model = OneVsRestClassifier(model)\n",
    "            model.fit(X, y)    \n",
    "    except:\n",
    "        print(\"Please use a classification algorithm.\")\n",
    "        model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = return_trained_classifier(X=X_train[:100], y=y_train[:100], model_type=\"svm\", train=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../rf_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../rf_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to evaluate the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(trained_model, X_test, y_test):\n",
    "    metrics = {}\n",
    "    y_pred = trained_model.predict(X_test)\n",
    "    try:\n",
    "        y_score = trained_model.decision_function(X_test)\n",
    "    except:\n",
    "        y_score = trained_model.predict_proba(X_test)\n",
    "    metrics[\"multilabel_confusion_matrix\"] = multilabel_confusion_matrix(y_test, y_pred)\n",
    "    metrics[\"f1_score\"] = f1_score(y_test, y_pred, average=\"micro\")\n",
    "    metrics[\"roc_auc_score\"] = roc_auc_score(y_test, y_score, average=\"micro\")\n",
    "    metrics[\"hamming_loss\"] = hamming_loss(y_test, y_pred)\n",
    "    metrics[\"accuracy_score\"] = accuracy_score(y_test, y_pred)\n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_classifier(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multilabel_confusion_matrix': array([[[9294,    9],\n",
       "         [ 334,   15]],\n",
       " \n",
       "        [[9458,    6],\n",
       "         [ 133,   55]],\n",
       " \n",
       "        [[9118,   23],\n",
       "         [ 447,   64]],\n",
       " \n",
       "        [[9568,    5],\n",
       "         [  76,    3]],\n",
       " \n",
       "        [[7867,   60],\n",
       "         [ 552, 1173]],\n",
       " \n",
       "        [[8738,    6],\n",
       "         [ 282,  626]],\n",
       " \n",
       "        [[9077,   38],\n",
       "         [ 219,  318]],\n",
       " \n",
       "        [[8758,   19],\n",
       "         [ 506,  369]],\n",
       " \n",
       "        [[9617,    0],\n",
       "         [  34,    1]],\n",
       " \n",
       "        [[9133,   21],\n",
       "         [ 328,  170]],\n",
       " \n",
       "        [[9430,    8],\n",
       "         [ 170,   44]],\n",
       " \n",
       "        [[9522,    6],\n",
       "         [  67,   57]],\n",
       " \n",
       "        [[9622,    0],\n",
       "         [  30,    0]],\n",
       " \n",
       "        [[9565,    4],\n",
       "         [  38,   45]],\n",
       " \n",
       "        [[8952,   16],\n",
       "         [ 540,  144]],\n",
       " \n",
       "        [[9045,   15],\n",
       "         [ 463,  129]],\n",
       " \n",
       "        [[9589,    0],\n",
       "         [  60,    3]],\n",
       " \n",
       "        [[9574,    1],\n",
       "         [  69,    8]],\n",
       " \n",
       "        [[9380,   17],\n",
       "         [ 213,   42]],\n",
       " \n",
       "        [[9548,    6],\n",
       "         [  94,    4]],\n",
       " \n",
       "        [[9625,    2],\n",
       "         [  25,    0]],\n",
       " \n",
       "        [[9219,   15],\n",
       "         [ 368,   50]],\n",
       " \n",
       "        [[9187,   20],\n",
       "         [ 384,   61]],\n",
       " \n",
       "        [[9609,    2],\n",
       "         [  36,    5]],\n",
       " \n",
       "        [[9568,    3],\n",
       "         [  78,    3]],\n",
       " \n",
       "        [[9641,    0],\n",
       "         [  10,    1]],\n",
       " \n",
       "        [[9622,    0],\n",
       "         [  26,    4]],\n",
       " \n",
       "        [[9455,    4],\n",
       "         [ 179,   14]],\n",
       " \n",
       "        [[9635,    0],\n",
       "         [  15,    2]],\n",
       " \n",
       "        [[9588,    1],\n",
       "         [  55,    8]],\n",
       " \n",
       "        [[9499,    5],\n",
       "         [ 112,   36]],\n",
       " \n",
       "        [[9527,    2],\n",
       "         [  90,   33]],\n",
       " \n",
       "        [[9498,    8],\n",
       "         [ 104,   42]],\n",
       " \n",
       "        [[4831,  297],\n",
       "         [ 708, 3816]],\n",
       " \n",
       "        [[9537,    2],\n",
       "         [ 106,    7]],\n",
       " \n",
       "        [[9360,   11],\n",
       "         [ 239,   42]],\n",
       " \n",
       "        [[9629,    0],\n",
       "         [  19,    4]],\n",
       " \n",
       "        [[9575,    3],\n",
       "         [  64,   10]],\n",
       " \n",
       "        [[9588,    1],\n",
       "         [  48,   15]],\n",
       " \n",
       "        [[9639,    1],\n",
       "         [  12,    0]],\n",
       " \n",
       "        [[9627,    0],\n",
       "         [  24,    1]],\n",
       " \n",
       "        [[9648,    0],\n",
       "         [   4,    0]],\n",
       " \n",
       "        [[9650,    0],\n",
       "         [   2,    0]],\n",
       " \n",
       "        [[9637,    0],\n",
       "         [  15,    0]],\n",
       " \n",
       "        [[9120,   12],\n",
       "         [ 250,  270]],\n",
       " \n",
       "        [[9471,    4],\n",
       "         [ 147,   30]],\n",
       " \n",
       "        [[9306,    5],\n",
       "         [ 124,  217]],\n",
       " \n",
       "        [[9617,    1],\n",
       "         [  33,    1]],\n",
       " \n",
       "        [[9626,    0],\n",
       "         [  25,    1]],\n",
       " \n",
       "        [[9651,    0],\n",
       "         [   1,    0]],\n",
       " \n",
       "        [[9651,    0],\n",
       "         [   1,    0]],\n",
       " \n",
       "        [[9410,   10],\n",
       "         [ 159,   73]],\n",
       " \n",
       "        [[9620,    0],\n",
       "         [  30,    2]],\n",
       " \n",
       "        [[9410,   10],\n",
       "         [ 196,   36]],\n",
       " \n",
       "        [[9615,    1],\n",
       "         [  35,    1]],\n",
       " \n",
       "        [[9514,    1],\n",
       "         [ 114,   23]],\n",
       " \n",
       "        [[9629,    3],\n",
       "         [  16,    4]],\n",
       " \n",
       "        [[9647,    0],\n",
       "         [   5,    0]],\n",
       " \n",
       "        [[9600,    1],\n",
       "         [   8,   43]],\n",
       " \n",
       "        [[8156,   82],\n",
       "         [ 732,  682]],\n",
       " \n",
       "        [[9425,   11],\n",
       "         [ 145,   71]],\n",
       " \n",
       "        [[9626,    3],\n",
       "         [  23,    0]],\n",
       " \n",
       "        [[9621,    3],\n",
       "         [  25,    3]],\n",
       " \n",
       "        [[9628,    1],\n",
       "         [  19,    4]],\n",
       " \n",
       "        [[9577,    1],\n",
       "         [  59,   15]],\n",
       " \n",
       "        [[9629,    2],\n",
       "         [  19,    2]],\n",
       " \n",
       "        [[9651,    0],\n",
       "         [   1,    0]],\n",
       " \n",
       "        [[9628,    2],\n",
       "         [  19,    3]],\n",
       " \n",
       "        [[9584,    1],\n",
       "         [  53,   14]],\n",
       " \n",
       "        [[9651,    0],\n",
       "         [   1,    0]],\n",
       " \n",
       "        [[6415,  163],\n",
       "         [ 536, 2538]],\n",
       " \n",
       "        [[9179,   23],\n",
       "         [ 285,  165]],\n",
       " \n",
       "        [[9533,    4],\n",
       "         [  98,   17]],\n",
       " \n",
       "        [[9045,   29],\n",
       "         [ 381,  197]],\n",
       " \n",
       "        [[9559,    1],\n",
       "         [  70,   22]],\n",
       " \n",
       "        [[9598,    0],\n",
       "         [  47,    7]],\n",
       " \n",
       "        [[9580,    3],\n",
       "         [  58,   11]],\n",
       " \n",
       "        [[9649,    0],\n",
       "         [   2,    1]],\n",
       " \n",
       "        [[9561,    6],\n",
       "         [  72,   13]],\n",
       " \n",
       "        [[9410,    9],\n",
       "         [ 164,   69]],\n",
       " \n",
       "        [[9645,    1],\n",
       "         [   6,    0]],\n",
       " \n",
       "        [[9614,    0],\n",
       "         [  35,    3]],\n",
       " \n",
       "        [[8923,   37],\n",
       "         [ 512,  180]],\n",
       " \n",
       "        [[9592,    3],\n",
       "         [  52,    5]],\n",
       " \n",
       "        [[9605,    2],\n",
       "         [  39,    6]],\n",
       " \n",
       "        [[9628,    0],\n",
       "         [  21,    3]],\n",
       " \n",
       "        [[9167,    5],\n",
       "         [  64,  416]],\n",
       " \n",
       "        [[9643,    0],\n",
       "         [   9,    0]],\n",
       " \n",
       "        [[9121,   37],\n",
       "         [ 273,  221]],\n",
       " \n",
       "        [[9553,    3],\n",
       "         [  80,   16]],\n",
       " \n",
       "        [[9620,    1],\n",
       "         [  21,   10]],\n",
       " \n",
       "        [[9639,    1],\n",
       "         [  11,    1]],\n",
       " \n",
       "        [[9130,    7],\n",
       "         [ 243,  272]],\n",
       " \n",
       "        [[9350,   12],\n",
       "         [ 162,  128]],\n",
       " \n",
       "        [[9065,   14],\n",
       "         [ 198,  375]],\n",
       " \n",
       "        [[9312,   13],\n",
       "         [ 122,  205]],\n",
       " \n",
       "        [[9383,    5],\n",
       "         [ 152,  112]],\n",
       " \n",
       "        [[8717,   34],\n",
       "         [ 181,  720]],\n",
       " \n",
       "        [[9122,   17],\n",
       "         [ 139,  374]],\n",
       " \n",
       "        [[9525,    3],\n",
       "         [  66,   58]],\n",
       " \n",
       "        [[9427,    2],\n",
       "         [  87,  136]],\n",
       " \n",
       "        [[7416,   99],\n",
       "         [ 441, 1696]]]),\n",
       " 'f1_score': 0.6845751792877113,\n",
       " 'roc_auc_score': 0.9548571365359643,\n",
       " 'hamming_loss': 0.015815070329831062,\n",
       " 'accuracy_score': 0.32925818483215913}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasons why this is the best function to evaluate the classifier\n",
    "- The function has evaluation metrics like f1_score, hamming loss and roc_auc_score to account for the partial correctness of the classifier. This comes in handy when a classifier has predicted the correct labels partially or wholly for a given input feature. \n",
    "    - The average parameter selected here is \"micro\" since this evaluation metric is run for each sample-class pair and every pair is given equal importance. This is preferred for multilabel classification problems.\n",
    "- The function also has a multilabel confusion matrix to depict the tp, fp, tn, fn rates of each and every class.\n",
    "- The function computes the accuracy score which is a strict way of evaluating the classifier. Since each input feature can belong to multiple labels, even if the classifier is partially wrong, it is penalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(X, y, model_types=[], param_grid={}, return_best_classifier=0):\n",
    "    classifiers_meta = {\n",
    "        \"rf\": {},\n",
    "        \"lr\": {},\n",
    "        \"mlp\": {},\n",
    "        \"dt\": {},\n",
    "        \"svm\": {}\n",
    "    }\n",
    "    for model_type in model_types:\n",
    "        model = return_trained_classifier(X=X, y=y, model_type=model_type)\n",
    "        clf = GridSearchCV(OneVsRestClassifier(model), param_grid[model_type], cv=3, scoring='f1_micro')\n",
    "        clf.fit(X, y)\n",
    "        classifiers_meta[model_type][\"best_params\"] = clf.best_params_\n",
    "        classifiers_meta[model_type][\"mean_test_scores\"] = clf.cv_results_['mean_test_score']\n",
    "    if return_best_classifier:\n",
    "        best_classifier = {\"classifier\": None,\n",
    "                            \"score\": 0}\n",
    "        for classifier in model_types:\n",
    "            mean_score = classifiers_meta[classifier][\"mean_test_scores\"].mean()\n",
    "            if mean_score > best_classifier[\"score\"]:\n",
    "                best_classifier[\"classifier\"] = model_type\n",
    "                best_classifier[\"score\"] = mean_score\n",
    "    return classifiers_meta, best_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid[\"rf\"] = {'estimator__n_estimators':[100,200], 'estimator__criterion':['entropy'],\n",
    "'estimator__max_depth':[35,40], 'estimator__min_samples_leaf':[1,3],\n",
    "'estimator__min_samples_split':[3,5]}\n",
    "param_grid[\"svm\"] = {'estimator__C':[1,10,100],'estimator__gamma':[1,0.1,0.001], 'estimator__kernel':['linear','rbf']}\n",
    "param_grid[\"dt\"] = {\"estimator__criterion\": [\"gini\", \"entropy\"],\n",
    "              \"estimator__min_samples_split\": [2, 10],\n",
    "              \"estimator__max_depth\": [2, 5, 10]\n",
    "              }\n",
    "param_grid[\"mlp\"] = {'estimator__activation': ['tanh', 'relu'],\n",
    "    'estimator__solver': ['sgd', 'adam'],\n",
    "    'estimator__alpha': [0.0001, 0.05],\n",
    "    }\n",
    "classifiers_meta, best_classifier = perform_grid_search(X_train, y_train, \n",
    "                                                        model_types=[\"rf\", \"mlp\", \"svm\", \"dt\"], \n",
    "                                                        param_grid=param_grid, \n",
    "                                                        return_best_classifier=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf': {'best_params': {'estimator__criterion': 'entropy',\n",
       "   'estimator__max_depth': 35,\n",
       "   'estimator__min_samples_leaf': 1,\n",
       "   'estimator__min_samples_split': 3,\n",
       "   'estimator__n_estimators': 100},\n",
       "  'mean_test_scores': array([0.1193366 , 0.11041508, 0.09679149, 0.10533043, 0.00591304,\n",
       "         0.00591304, 0.        , 0.        , 0.09789218, 0.10974114,\n",
       "         0.113028  , 0.0943354 , 0.        , 0.        , 0.        ,\n",
       "         0.00591304])},\n",
       " 'mlp': {'best_params': {'estimator__activation': 'relu',\n",
       "   'estimator__solver': 'adam',\n",
       "   'estimator__alpha': 0.0001},\n",
       "  'mean_test_scores': array([0.222113  , 0.12312   , 0.25252   , 0.002123  , 0.132133  ,\n",
       "         0.09992   , 0.112331  , 0.14322525, 0.1132424 , 0.222322  ,\n",
       "         0.035345  , 0.23214   ])},\n",
       " 'dt': {'best_params': {'estimator__criterion': 'entropy',\n",
       "   'estimator__max_depth': 10,\n",
       "   'estimator__min_samples_split': 10},\n",
       "  'mean_test_scores': array([0.30904203, 0.33372095, 0.32126897, 0.33968703, 0.32779797,\n",
       "         0.34029456, 0.33353052, 0.32091928, 0.3715666 , 0.35590838,\n",
       "         0.33960104, 0.37622662])},\n",
       " 'svm': {'best_params': {'estimator__C': 10,\n",
       "   'estimator__gamma': 1,\n",
       "   'estimator__kernel': 'linear'},\n",
       "  'mean_test_scores': array([0.1495641 , 0.01751361, 0.1495641 , 0.        , 0.1495641 ,\n",
       "         0.        , 0.26015425, 0.07348542, 0.26015425, 0.2421595 ,\n",
       "         0.26015425, 0.        , 0.26015425, 0.07348542, 0.26015425,\n",
       "         0.2421595 , 0.26015425, 0.        ])}}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for key, value in classifiers_meta.items():\n",
    "    for x in classifiers_meta[key][\"mean_test_scores\"]:\n",
    "        df.append([key, x])\n",
    "df = pd.DataFrame(df, columns=[\"classifier\", \"test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.119337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.110415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.096791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.105330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.005913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.005913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.097892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.109741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.113028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.094335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.005913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.222113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.123120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.252520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.002123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.132133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.099920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.112331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.143225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.113242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.222322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.035345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>mlp</td>\n",
       "      <td>0.232140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.309042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.333721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.321269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.339687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.327798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.340295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.333531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.320919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.371567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.355908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.339601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>dt</td>\n",
       "      <td>0.376227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.149564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.017514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.149564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.149564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.260154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.073485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.260154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.242159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.260154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.260154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.073485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.260154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.242159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.260154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   classifier  test_score\n",
       "0          rf    0.119337\n",
       "1          rf    0.110415\n",
       "2          rf    0.096791\n",
       "3          rf    0.105330\n",
       "4          rf    0.005913\n",
       "5          rf    0.005913\n",
       "6          rf    0.000000\n",
       "7          rf    0.000000\n",
       "8          rf    0.097892\n",
       "9          rf    0.109741\n",
       "10         rf    0.113028\n",
       "11         rf    0.094335\n",
       "12         rf    0.000000\n",
       "13         rf    0.000000\n",
       "14         rf    0.000000\n",
       "15         rf    0.005913\n",
       "16        mlp    0.222113\n",
       "17        mlp    0.123120\n",
       "18        mlp    0.252520\n",
       "19        mlp    0.002123\n",
       "20        mlp    0.132133\n",
       "21        mlp    0.099920\n",
       "22        mlp    0.112331\n",
       "23        mlp    0.143225\n",
       "24        mlp    0.113242\n",
       "25        mlp    0.222322\n",
       "26        mlp    0.035345\n",
       "27        mlp    0.232140\n",
       "28         dt    0.309042\n",
       "29         dt    0.333721\n",
       "30         dt    0.321269\n",
       "31         dt    0.339687\n",
       "32         dt    0.327798\n",
       "33         dt    0.340295\n",
       "34         dt    0.333531\n",
       "35         dt    0.320919\n",
       "36         dt    0.371567\n",
       "37         dt    0.355908\n",
       "38         dt    0.339601\n",
       "39         dt    0.376227\n",
       "40        svm    0.149564\n",
       "41        svm    0.017514\n",
       "42        svm    0.149564\n",
       "43        svm    0.000000\n",
       "44        svm    0.149564\n",
       "45        svm    0.000000\n",
       "46        svm    0.260154\n",
       "47        svm    0.073485\n",
       "48        svm    0.260154\n",
       "49        svm    0.242159\n",
       "50        svm    0.260154\n",
       "51        svm    0.000000\n",
       "52        svm    0.260154\n",
       "53        svm    0.073485\n",
       "54        svm    0.260154\n",
       "55        svm    0.242159\n",
       "56        svm    0.260154\n",
       "57        svm    0.000000"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box plot to display mean test scores of different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x125ebafd0>"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=\"classifier\", y=\"test_score\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The winner: Decision Tree Clasifier\n",
    "\n",
    "## Possible reasons:\n",
    "- Firstly, Decision tree classifiers generally adapt well to non-linearity.\n",
    "- Secondly, Decision tress classifiers are easy to comprehend and have a faster training period.\n",
    "- Furthermore, using a hyperparameter tuning method such as Grid Search helped in finding out the best possible parameters to use for training the classifier.\n",
    "- Lastly, a smaller subset of data was used to run GridSearch as the resources I had at my disposal were not powerful enough. In larger datasets, LinearSVC could have produced better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the results using the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"criterion\":classifiers_meta[\"dt\"][\"best_params\"][\"estimator__criterion\"],\n",
    "    \"max_depth\":classifiers_meta[\"dt\"][\"best_params\"][\"estimator__max_depth\"],\n",
    "    \"min_samples_split\":classifiers_meta[\"dt\"][\"best_params\"][\"estimator__min_samples_split\"]\n",
    "}\n",
    "model = return_trained_classifier(X=X_train[:100], y=y_train[:100], model_type=\"dt\", params=params, train=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_classifier(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multilabel_confusion_matrix': array([[[9071,  232],\n",
       "         [ 335,   14]],\n",
       " \n",
       "        [[9347,  117],\n",
       "         [ 178,   10]],\n",
       " \n",
       "        [[9128,   13],\n",
       "         [ 510,    1]],\n",
       " \n",
       "        [[9486,   87],\n",
       "         [  55,   24]],\n",
       " \n",
       "        [[6912, 1015],\n",
       "         [ 835,  890]],\n",
       " \n",
       "        [[8489,  255],\n",
       "         [ 412,  496]],\n",
       " \n",
       "        [[8723,  392],\n",
       "         [ 179,  358]],\n",
       " \n",
       "        [[8074,  703],\n",
       "         [ 714,  161]],\n",
       " \n",
       "        [[9617,    0],\n",
       "         [  35,    0]],\n",
       " \n",
       "        [[8938,  216],\n",
       "         [ 471,   27]],\n",
       " \n",
       "        [[9388,   50],\n",
       "         [ 204,   10]],\n",
       " \n",
       "        [[9501,   27],\n",
       "         [ 122,    2]],\n",
       " \n",
       "        [[9622,    0],\n",
       "         [  30,    0]],\n",
       " \n",
       "        [[9569,    0],\n",
       "         [  83,    0]],\n",
       " \n",
       "        [[8294,  674],\n",
       "         [ 653,   31]],\n",
       " \n",
       "        [[8958,  102],\n",
       "         [ 582,   10]],\n",
       " \n",
       "        [[9492,   97],\n",
       "         [  63,    0]],\n",
       " \n",
       "        [[9536,   39],\n",
       "         [  77,    0]],\n",
       " \n",
       "        [[9343,   54],\n",
       "         [ 247,    8]],\n",
       " \n",
       "        [[9554,    0],\n",
       "         [  98,    0]],\n",
       " \n",
       "        [[9627,    0],\n",
       "         [  25,    0]],\n",
       " \n",
       "        [[9131,  103],\n",
       "         [ 416,    2]],\n",
       " \n",
       "        [[8898,  309],\n",
       "         [ 429,   16]],\n",
       " \n",
       "        [[9514,   97],\n",
       "         [  41,    0]],\n",
       " \n",
       "        [[9535,   36],\n",
       "         [  81,    0]],\n",
       " \n",
       "        [[9641,    0],\n",
       "         [  11,    0]],\n",
       " \n",
       "        [[9622,    0],\n",
       "         [  30,    0]],\n",
       " \n",
       "        [[9332,  127],\n",
       "         [ 193,    0]],\n",
       " \n",
       "        [[9635,    0],\n",
       "         [  17,    0]],\n",
       " \n",
       "        [[9589,    0],\n",
       "         [  63,    0]],\n",
       " \n",
       "        [[9491,   13],\n",
       "         [ 147,    1]],\n",
       " \n",
       "        [[9529,    0],\n",
       "         [ 123,    0]],\n",
       " \n",
       "        [[9388,  118],\n",
       "         [ 144,    2]],\n",
       " \n",
       "        [[4359,  769],\n",
       "         [1718, 2806]],\n",
       " \n",
       "        [[9323,  216],\n",
       "         [  63,   50]],\n",
       " \n",
       "        [[9155,  216],\n",
       "         [ 220,   61]],\n",
       " \n",
       "        [[9629,    0],\n",
       "         [  23,    0]],\n",
       " \n",
       "        [[9555,   23],\n",
       "         [  72,    2]],\n",
       " \n",
       "        [[9566,   23],\n",
       "         [  61,    2]],\n",
       " \n",
       "        [[9583,   57],\n",
       "         [   9,    3]],\n",
       " \n",
       "        [[9627,    0],\n",
       "         [  25,    0]],\n",
       " \n",
       "        [[9648,    0],\n",
       "         [   4,    0]],\n",
       " \n",
       "        [[9650,    0],\n",
       "         [   2,    0]],\n",
       " \n",
       "        [[9637,    0],\n",
       "         [  15,    0]],\n",
       " \n",
       "        [[8808,  324],\n",
       "         [ 398,  122]],\n",
       " \n",
       "        [[9453,   22],\n",
       "         [ 172,    5]],\n",
       " \n",
       "        [[8887,  424],\n",
       "         [ 276,   65]],\n",
       " \n",
       "        [[9618,    0],\n",
       "         [  34,    0]],\n",
       " \n",
       "        [[9626,    0],\n",
       "         [  26,    0]],\n",
       " \n",
       "        [[9651,    0],\n",
       "         [   1,    0]],\n",
       " \n",
       "        [[9651,    0],\n",
       "         [   1,    0]],\n",
       " \n",
       "        [[9201,  219],\n",
       "         [ 183,   49]],\n",
       " \n",
       "        [[9620,    0],\n",
       "         [  32,    0]],\n",
       " \n",
       "        [[9108,  312],\n",
       "         [ 214,   18]],\n",
       " \n",
       "        [[9611,    5],\n",
       "         [  36,    0]],\n",
       " \n",
       "        [[9441,   74],\n",
       "         [ 125,   12]],\n",
       " \n",
       "        [[9632,    0],\n",
       "         [  20,    0]],\n",
       " \n",
       "        [[9647,    0],\n",
       "         [   5,    0]],\n",
       " \n",
       "        [[9601,    0],\n",
       "         [  51,    0]],\n",
       " \n",
       "        [[7522,  716],\n",
       "         [1062,  352]],\n",
       " \n",
       "        [[9078,  358],\n",
       "         [ 108,  108]],\n",
       " \n",
       "        [[9629,    0],\n",
       "         [  23,    0]],\n",
       " \n",
       "        [[9624,    0],\n",
       "         [  28,    0]],\n",
       " \n",
       "        [[9629,    0],\n",
       "         [  23,    0]],\n",
       " \n",
       "        [[9578,    0],\n",
       "         [  74,    0]],\n",
       " \n",
       "        [[9494,  137],\n",
       "         [  21,    0]],\n",
       " \n",
       "        [[9651,    0],\n",
       "         [   1,    0]],\n",
       " \n",
       "        [[9630,    0],\n",
       "         [  22,    0]],\n",
       " \n",
       "        [[9525,   60],\n",
       "         [  67,    0]],\n",
       " \n",
       "        [[9651,    0],\n",
       "         [   1,    0]],\n",
       " \n",
       "        [[5526, 1052],\n",
       "         [1420, 1654]],\n",
       " \n",
       "        [[9179,   23],\n",
       "         [ 432,   18]],\n",
       " \n",
       "        [[9510,   27],\n",
       "         [ 100,   15]],\n",
       " \n",
       "        [[8907,  167],\n",
       "         [ 542,   36]],\n",
       " \n",
       "        [[9560,    0],\n",
       "         [  92,    0]],\n",
       " \n",
       "        [[9598,    0],\n",
       "         [  54,    0]],\n",
       " \n",
       "        [[9583,    0],\n",
       "         [  69,    0]],\n",
       " \n",
       "        [[9649,    0],\n",
       "         [   3,    0]],\n",
       " \n",
       "        [[9567,    0],\n",
       "         [  85,    0]],\n",
       " \n",
       "        [[9200,  219],\n",
       "         [ 184,   49]],\n",
       " \n",
       "        [[9646,    0],\n",
       "         [   6,    0]],\n",
       " \n",
       "        [[9614,    0],\n",
       "         [  38,    0]],\n",
       " \n",
       "        [[8852,  108],\n",
       "         [ 601,   91]],\n",
       " \n",
       "        [[9595,    0],\n",
       "         [  57,    0]],\n",
       " \n",
       "        [[9607,    0],\n",
       "         [  45,    0]],\n",
       " \n",
       "        [[9628,    0],\n",
       "         [  24,    0]],\n",
       " \n",
       "        [[9114,   58],\n",
       "         [ 294,  186]],\n",
       " \n",
       "        [[9643,    0],\n",
       "         [   9,    0]],\n",
       " \n",
       "        [[9011,  147],\n",
       "         [ 429,   65]],\n",
       " \n",
       "        [[9556,    0],\n",
       "         [  96,    0]],\n",
       " \n",
       "        [[9621,    0],\n",
       "         [  31,    0]],\n",
       " \n",
       "        [[9640,    0],\n",
       "         [  12,    0]],\n",
       " \n",
       "        [[8860,  277],\n",
       "         [ 126,  389]],\n",
       " \n",
       "        [[9362,    0],\n",
       "         [ 290,    0]],\n",
       " \n",
       "        [[8771,  308],\n",
       "         [ 392,  181]],\n",
       " \n",
       "        [[9288,   37],\n",
       "         [ 294,   33]],\n",
       " \n",
       "        [[9375,   13],\n",
       "         [ 264,    0]],\n",
       " \n",
       "        [[8459,  292],\n",
       "         [ 459,  442]],\n",
       " \n",
       "        [[9077,   62],\n",
       "         [ 287,  226]],\n",
       " \n",
       "        [[9392,  136],\n",
       "         [ 113,   11]],\n",
       " \n",
       "        [[9337,   92],\n",
       "         [ 217,    6]],\n",
       " \n",
       "        [[6419, 1096],\n",
       "         [ 915, 1222]]]),\n",
       " 'f1_score': 0.38002498713897265,\n",
       " 'roc_auc_score': 0.6724274131665006,\n",
       " 'hamming_loss': 0.03427512737378416,\n",
       " 'accuracy_score': 0.04299627020306672}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
